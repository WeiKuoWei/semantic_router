import numpy as np
import logging
import re
import time
import atexit
import gc
from typing import Dict, Any, Callable
from semantic_router.encoders import OpenAIEncoder
from sentence_transformers import SentenceTransformer, util

from utils.chromadb_handler import ChromaDBHandler
from utils.llm_service import generate_response
from utils.config import DEFAULT_EMBEDDING_MODEL

# Import centroid vectors (will be generated by the visualizer)
try:
    from router.centroid_vectors import GROUP_CENTROIDS, EXPERT_CENTROIDS, GROUP_TO_EXPERTS
except ImportError:
    # Default empty values if file doesn't exist yet
    GROUP_CENTROIDS = {}
    EXPERT_CENTROIDS = {}
    EXPERT_TO_GROUP = {}
    GROUP_TO_EXPERTS = {}

# Get a named logger instead of using basicConfig
logger = logging.getLogger("multi_layer_router")

# Define a custom route class that stores a centroid
class CentroidRoute:
    def __init__(self, name: str, centroid: np.ndarray):
        self.name = name
        self.centroid = centroid

class MultiLayerRouter:
    """
    A two-layer semantic router that first routes to a group,
    then to a specific expert within that group.
    """
    
    def __init__(self, use_openai: bool = False, top_k: int = 3, session_manager=None):
        """
        Initialize the MultiLayerRouter.
        
        Args:
            use_openai: Whether to use OpenAI for embeddings (True) or SentenceTransformer (False)
            top_k: Number of documents to retrieve from the vector database
            session_manager: Optional session manager to maintain conversation history
        """
        self.use_openai = use_openai
        self.top_k = top_k
        self.db_handler = ChromaDBHandler()
        self.session_manager = session_manager
        
        if use_openai:
            self.encoder = OpenAIEncoder()
        else:
            self.model = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)
            
            # Register cleanup function for SentenceTransformer model
            def cleanup_model():
                logger.info("Cleaning up SentenceTransformer model...")
                try:
                    # Delete the model and force garbage collection
                    del self.model
                    gc.collect()
                    logger.info("SentenceTransformer model cleanup complete")
                except Exception as e:
                    logger.error(f"Error during model cleanup: {e}")
            
            # Register the cleanup function to run at program exit
            atexit.register(cleanup_model)
        
        # Set up the routes
        self.group_routes = self._setup_group_routes()
        self.expert_routes = self._setup_expert_routes()
        
        # Response functions mapping
        self.expert_responses = {}
        self.fallback_response = self._fallback_response
        
        # Command for clearing session
        self.reset_command_pattern = re.compile(r'^(new_session|reset|clear)$', re.IGNORECASE)
        
        # Also register cleanup for ChromaDB client
        def cleanup_db():
            logger.info("Cleaning up ChromaDB client...")
            try:
                if hasattr(self.db_handler, 'client') and self.db_handler.client is not None:
                    # Some versions of ChromaDB have a close method
                    if hasattr(self.db_handler.client, 'close'):
                        self.db_handler.client.close()
                    # Force garbage collection
                    del self.db_handler
                    gc.collect()
                    logger.info("ChromaDB client cleanup complete")
            except Exception as e:
                logger.error(f"Error during ChromaDB cleanup: {e}")
        
        atexit.register(cleanup_db)
    
    def _setup_group_routes(self) -> Dict[str, CentroidRoute]:
        """
        Set up the group-level routes based on centroids.
        
        Returns:
            Dictionary mapping group names to CentroidRoute objects
        """
        routes = {}
        
        for group_name, centroid in GROUP_CENTROIDS.items():
            # Create a CentroidRoute for this group
            route = CentroidRoute(
                name=group_name,
                centroid=np.array(centroid)
            )
            routes[group_name] = route
        
        logger.info(f"Set up {len(routes)} group routes")
        return routes
    
    def _setup_expert_routes(self) -> Dict[str, Dict[str, CentroidRoute]]:
        """
        Set up the expert-level routes based on centroids.
        
        Returns:
            Nested dictionary mapping group names to expert routes
        """
        routes = {}
        
        for group_name, experts in GROUP_TO_EXPERTS.items():
            routes[group_name] = {}
            
            for expert_name in experts:
                if expert_name in EXPERT_CENTROIDS:
                    # Create a CentroidRoute for this expert
                    route = CentroidRoute(
                        name=expert_name,
                        centroid=np.array(EXPERT_CENTROIDS[expert_name])
                    )
                    routes[group_name][expert_name] = route
        
        expert_count = sum(len(group_experts) for group_experts in routes.values())
        logger.info(f"Set up {expert_count} expert routes across {len(routes)} groups")
        return routes
    
    def register_expert_response(self, expert_name: str, response_func: Callable):
        """
        Register a response function for a specific expert.
        
        Args:
            expert_name: Name of the expert
            response_func: Function to handle queries for this expert
        """
        self.expert_responses[expert_name] = response_func
    
    async def _fallback_response(self, query: str):
        """
        Default fallback response when no matching expert is found.
        
        Args:
            query: The user query
            
        Returns:
            A fallback response
        """
        return {
            "answer": "I'm not sure which expert can best help with that question. "
                     "Could you please provide more specific details?"
        }
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """
        Get an embedding for the input text.
        
        Args:
            text: Input text to encode
            
        Returns:
            Embedding vector as numpy array
        """
        if self.use_openai:
            # Use OpenAI's encoder
            return self.encoder.encode(text)
        else:
            # Use SentenceTransformer
            return self.model.encode(text)
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors.
        
        Args:
            vec1: First vector
            vec2: Second vector
            
        Returns:
            Cosine similarity score
        """
        # Ensure both vectors have the same dtype (float32)
        vec1 = vec1.astype(np.float32)
        vec2 = vec2.astype(np.float32)
        return float(util.cos_sim(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0])
    
    async def route_query(self, query: str, user_id: str = "default_user") -> Dict[str, Any]:
        """
        Route a query through the two-layer system:
        1. Check for session reset command
        2. Find the best expert considering conversation history
        3. Retrieve relevant documents
        4. Generate response using LLM with conversation context
        
        Args:
            query: The user query
            user_id: Identifier for the user (for session tracking)
            
        Returns:
            Response from the selected expert

        Note:
        1. Update the `fallback_response` to directing the query to the default/general expert. 
        2. If no expert group is found, direct query to fallback_response and skip the second layer.
        3. Update the similarity threshold to [placeholder] to avoid false positives.
        4. At the moment, conversation_context (includes only user queries) is used for routing instead of conversation_history.
        """
        # Check for session reset command
        if self.reset_command_pattern.match(query.strip()):
            if self.session_manager:
                self.session_manager.clear_session(user_id)
            return {"answer": "Session has been reset. What would you like to ask about?"}
        
        # Get conversation context if available
        conversation_context = ""
        if self.session_manager:
            conversation_context = self.session_manager.get_conversation_context(user_id)
        
        # Create augmented query with conversation context
        augmented_query = query
        if conversation_context:
            '''***
            Consider implementing diminishing returns for conversation context length. Older context should be discounted. 
            '''
            augmented_query = f"{conversation_context} {query} {query}"
        
        # Get query embedding for the augmented query
        query_embedding = await self.get_embedding(augmented_query)
        
        # First-layer routing: find the best group
        best_group = None
        best_similarity = -1
        
        for group_name, route in self.group_routes.items():
            similarity = self.cosine_similarity(query_embedding, route.centroid)
            logger.info(f"Group '{group_name}' similarity: {similarity:.4f}")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_group = group_name
        
        if best_group is None:
            logger.warning("No matching group found")
            return await self.fallback_response(query)
        
        logger.info(f"Selected group: {best_group} (similarity: {best_similarity:.4f})")
        
        # Second-layer routing: find the best expert within the group
        best_expert = None
        best_similarity = -1
        
        start_time = time.time()
        for expert_name, route in self.expert_routes.get(best_group, {}).items():
            similarity = self.cosine_similarity(query_embedding, route.centroid)
            logger.info(f"Expert '{expert_name}' similarity: {similarity:.4f}")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_expert = expert_name
        
        if best_expert is None:
            logger.warning(f"No matching expert found in group {best_group}")
            return await self.fallback_response(query)
        
        logger.info(f"Selected expert: {best_expert} (similarity: {best_similarity:.4f})")
        # log info in milliseconds
        logger.info(f"Time taken for routing: {(time.time() - start_time) * 1000:.2f} ms")
        
        # Get conversation history for the response
        conversation_history = ""
        if self.session_manager:
            conversation_history = self.session_manager.get_formatted_history(user_id)
        
        # Check if custom response function is registered
        if best_expert in self.expert_responses:
            # Use custom response function (Note: custom handlers would need to be updated 
            # to use conversation history as well, but that's outside our scope here)
            response = await self.expert_responses[best_expert](query)
        else:
            # Default RAG pipeline:
            # 1. Retrieve relevant documents from ChromaDB
            results = await self.db_handler.get_similar_documents(best_expert, query_embedding.tolist(), self.top_k)
            
            # 2. Extract documents
            documents = results.get("documents", [])
            
            if not documents:
                logger.warning(f"No documents found for expert {best_expert}")
                response = await self.fallback_response(query)
            else:
                # 3. Generate response using LLM with conversation history
                response = await generate_response(
                    query=query, 
                    context=documents,
                    conversation_history=conversation_history,
                    expert_name=best_expert, 
                )
        
        # Record the interaction in the session manager
        if self.session_manager:
            self.session_manager.add_message(
                user_id=user_id, 
                query=query, 
                response=response, 
                conversation_context=conversation_context,
                documents=documents,
                expert_name=best_expert,
                )
        
        return response, best_expert