import numpy as np
import logging
import re
import time
import atexit
import gc
from typing import Dict, Any, Callable, Tuple, List
from semantic_router.encoders import OpenAIEncoder
from sentence_transformers import SentenceTransformer, util

from utils.chromadb_handler import ChromaDBHandler
from utils.llm_service import generate_response
from utils.config import DEFAULT_EMBEDDING_MODEL

# Import centroid vectors (will be generated by the visualizer)
try:
    from router.centroid_vectors import GROUP_CENTROIDS, EXPERT_CENTROIDS, GROUP_TO_EXPERTS
except ImportError:
    # Default empty values if file doesn't exist yet
    GROUP_CENTROIDS = {}
    EXPERT_CENTROIDS = {}
    EXPERT_TO_GROUP = {}
    GROUP_TO_EXPERTS = {}

# Get a named logger instead of using basicConfig
logger = logging.getLogger("multi_layer_router")

# Define a custom route class that stores a centroid
class CentroidRoute:
    def __init__(self, name: str, centroid: np.ndarray):
        self.name = name
        self.centroid = centroid

class MultiLayerRouter:
    """
    A two-layer semantic router that first routes to a group,
    then to a specific expert within that group.
    """
    
    def __init__(self, use_openai: bool = False, top_k: int = 3, session_manager=None):
        """
        Initialize the MultiLayerRouter.
        
        Args:
            use_openai: Whether to use OpenAI for embeddings (True) or SentenceTransformer (False)
            top_k: Number of documents to retrieve from the vector database
            session_manager: Optional session manager to maintain conversation history
        """
        self.use_openai = use_openai
        self.top_k = top_k
        self.db_handler = ChromaDBHandler()
        self.session_manager = session_manager
        
        if use_openai:
            self.encoder = OpenAIEncoder()
        else:
            self.model = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)
            
            # Register cleanup function for SentenceTransformer model
            def cleanup_model():
                logger.info("Cleaning up SentenceTransformer model...")
                try:
                    # Delete the model and force garbage collection
                    del self.model
                    gc.collect()
                    logger.info("SentenceTransformer model cleanup complete")
                except Exception as e:
                    logger.error(f"Error during model cleanup: {e}")
            
            # Register the cleanup function to run at program exit
            atexit.register(cleanup_model)
        
        # Set up the routes
        self.group_routes = self._setup_group_routes()
        self.expert_routes = self._setup_expert_routes()
        
        # Response functions mapping
        self.expert_responses = {}
        self.fallback_response = self._fallback_response
        
        # Command for clearing session
        self.reset_command_pattern = re.compile(r'^(new_session|reset|clear)$', re.IGNORECASE)
        
        # Also register cleanup for ChromaDB client
        def cleanup_db():
            logger.info("Cleaning up ChromaDB client...")
            try:
                if hasattr(self.db_handler, 'client') and self.db_handler.client is not None:
                    # Some versions of ChromaDB have a close method
                    if hasattr(self.db_handler.client, 'close'):
                        self.db_handler.client.close()
                    # Force garbage collection
                    del self.db_handler
                    gc.collect()
                    logger.info("ChromaDB client cleanup complete")
            except Exception as e:
                logger.error(f"Error during ChromaDB cleanup: {e}")
        
        atexit.register(cleanup_db)
    
    def _setup_group_routes(self) -> Dict[str, CentroidRoute]:
        """
        Set up the group-level routes based on centroids.
        
        Returns:
            Dictionary mapping group names to CentroidRoute objects
        """
        routes = {}
        
        for group_name, centroid in GROUP_CENTROIDS.items():
            # Create a CentroidRoute for this group
            route = CentroidRoute(
                name=group_name,
                centroid=np.array(centroid)
            )
            routes[group_name] = route
        
        logger.info(f"Set up {len(routes)} group routes")
        return routes
    
    def _setup_expert_routes(self) -> Dict[str, Dict[str, CentroidRoute]]:
        """
        Set up the expert-level routes based on centroids.
        
        Returns:
            Nested dictionary mapping group names to expert routes
        """
        routes = {}
        
        for group_name, experts in GROUP_TO_EXPERTS.items():
            routes[group_name] = {}
            
            for expert_name in experts:
                if expert_name in EXPERT_CENTROIDS:
                    # Create a CentroidRoute for this expert
                    route = CentroidRoute(
                        name=expert_name,
                        centroid=np.array(EXPERT_CENTROIDS[expert_name])
                    )
                    routes[group_name][expert_name] = route
        
        expert_count = sum(len(group_experts) for group_experts in routes.values())
        logger.info(f"Set up {expert_count} expert routes across {len(routes)} groups")
        return routes
    
    def register_expert_response(self, expert_name: str, response_func: Callable):
        """
        Register a response function for a specific expert.
        
        Args:
            expert_name: Name of the expert
            response_func: Function to handle queries for this expert
        """
        self.expert_responses[expert_name] = response_func
    
    async def _fallback_response(self, query: str):
        """
        Default fallback response when no matching expert is found.
        
        Args:
            query: The user query
            
        Returns:
            A fallback response
        """
        return {
            "answer": "I'm not sure which expert can best help with that question. "
                     "Could you please provide more specific details?"
        }
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """
        Get an embedding for the input text.
        
        Args:
            text: Input text to encode
            
        Returns:
            Embedding vector as numpy array
        """
        if self.use_openai:
            # Use OpenAI's encoder
            return self.encoder.encode(text)
        else:
            # Use SentenceTransformer
            return self.model.encode(text)
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors.
        
        Args:
            vec1: First vector
            vec2: Second vector
            
        Returns:
            Cosine similarity score
        """
        # Ensure both vectors have the same dtype (float32)
        vec1 = vec1.astype(np.float32)
        vec2 = vec2.astype(np.float32)
        return float(util.cos_sim(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0])
    
    async def route_query(self, query: str, user_id: str = "default_user") -> Tuple[Dict[str, Any], str]:
        """
        Route a query through the two-layer system:
        1. Check for session reset command
        2. Route query to appropriate expert group, then specific expert
        3. Retrieve relevant documents and generate response
        
        Args:
            query: The user query
            user_id: Identifier for the user (for session tracking)
            
        Returns:
            Tuple of (response, expert_name) where response is a dict with answer
        """
        # Check for session reset command
        if self.reset_command_pattern.match(query.strip()):
            if self.session_manager:
                self.session_manager.clear_session(user_id)
            return {"answer": "Session has been reset. What would you like to ask about?"}
        
        # Get query embedding
        query_embedding = await self.get_embedding(query)
        
        # First-layer routing: find the best group
        best_group, group_similarity = self._find_best_match(
            query_embedding, 
            self.group_routes,
            "group"
        )
        
        if best_group is None:
            logger.warning("No matching group found")
            return await self.fallback_response(query)
        
        # Second-layer routing: find the best expert within the group
        start_time = time.time()
        best_expert, expert_similarity = self._find_best_match(
            query_embedding, 
            self.expert_routes.get(best_group, {}),
            "expert"
        )
        
        if best_expert is None:
            logger.warning(f"No matching expert found in group {best_group}")
            return await self.fallback_response(query)
        
        logger.info(f"Time taken for routing: {(time.time() - start_time) * 1000:.2f} ms")
        
        # # Get conversation history for the response
        # conversation_history = ""
        # if self.session_manager:
        #     conversation_history = self.session_manager.get_formatted_history(user_id)
        
        # Generate response using appropriate method
        response, documents = await self._generate_expert_response(
            query, 
            best_expert, 
            query_embedding, 
            conversation_history="" # placeholder for conversation history
        )
        
        # Record the interaction in the session manager
        if self.session_manager:
            self.session_manager.add_message(
                user_id=user_id, 
                query=query, 
                response=response, 
                conversation_context="", # placeholder for conversation context
                documents=documents,
                expert_name=best_expert,
            )
        
        return response, best_expert

    def _find_best_match(self, query_embedding, routes, entity_type="group"):
        """
        Helper method to find the best matching route based on cosine similarity.
        
        Args:
            query_embedding: The query embedding vector
            routes: Dictionary of routes to compare against
            entity_type: Type of entity being matched (for logging)
            
        Returns:
            Tuple of (best_match_name, best_similarity)
        """
        best_match = None
        best_similarity = -1
        
        for name, route in routes.items():
            similarity = self.cosine_similarity(query_embedding, route.centroid)
            logger.info(f"{entity_type.title()} '{name}' similarity: {similarity:.4f}")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = name
        
        if best_match:
            logger.info(f"Selected {entity_type}: {best_match} (similarity: {best_similarity:.4f})")
            
        return best_match, best_similarity

    async def _generate_expert_response(self, query, expert_name, query_embedding, conversation_history):
        """
        Generate a response using either a custom expert function or the default RAG pipeline.
        
        Args:
            query: The user query
            expert_name: Name of the selected expert
            query_embedding: The query embedding vector
            conversation_history: Formatted conversation history
            
        Returns:
            Tuple of (response_dict, documents)
        """
        # Check if custom response function is registered
        if expert_name in self.expert_responses:
            return await self.expert_responses[expert_name](query), []
        
        # Default RAG pipeline
        results = await self.db_handler.get_similar_documents(
            expert_name, 
            query_embedding.tolist(), 
            self.top_k
        )
        
        documents = results.get("documents", [])
        
        if not documents:
            logger.warning(f"No documents found for expert {expert_name}")
            return await self.fallback_response(query), []
        
        # Generate response using LLM with conversation history
        response = await generate_response(
            query=query, 
            context=documents,
            conversation_history=conversation_history,
            expert_name=expert_name, 
        )
        
        return response, documents