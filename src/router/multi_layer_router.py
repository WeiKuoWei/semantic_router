import numpy as np
import logging
import time
from typing import Dict, List, Any, Callable, Optional
from semantic_router import Route
from semantic_router.encoders import OpenAIEncoder
from sentence_transformers import SentenceTransformer, util

from utils.chromadb_handler import ChromaDBHandler
from utils.llm_service import generate_response

# Import centroid vectors (will be generated by the visualizer)
try:
    from router.centroid_vectors import GROUP_CENTROIDS, EXPERT_CENTROIDS, EXPERT_TO_GROUP, GROUP_TO_EXPERTS
except ImportError:
    # Default empty values if file doesn't exist yet
    GROUP_CENTROIDS = {}
    EXPERT_CENTROIDS = {}
    EXPERT_TO_GROUP = {}
    GROUP_TO_EXPERTS = {}

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Define a custom route class that stores a centroid
class CentroidRoute:
    def __init__(self, name: str, centroid: np.ndarray):
        self.name = name
        self.centroid = centroid


class MultiLayerRouter:
    """
    A two-layer semantic router that first routes to a group,
    then to a specific expert within that group.
    """
    
    def __init__(self, use_openai: bool = False, top_k: int = 3):
        """
        Initialize the MultiLayerRouter.
        
        Args:
            use_openai: Whether to use OpenAI for embeddings (True) or SentenceTransformer (False)
            top_k: Number of documents to retrieve from the vector database
        """
        self.use_openai = use_openai
        self.top_k = top_k
        self.db_handler = ChromaDBHandler()
        
        if use_openai:
            self.encoder = OpenAIEncoder()
        else:
            self.model = SentenceTransformer('all-mpnet-base-v2')
        
        # Set up the routes
        self.group_routes = self._setup_group_routes()
        self.expert_routes = self._setup_expert_routes()
        
        # Response functions mapping
        self.expert_responses = {}
        self.fallback_response = self._fallback_response
    
    def _setup_group_routes(self) -> Dict[str, CentroidRoute]:
        """
        Set up the group-level routes based on centroids.
        
        Returns:
            Dictionary mapping group names to CentroidRoute objects
        """
        routes = {}
        
        for group_name, centroid in GROUP_CENTROIDS.items():
            # Create a CentroidRoute for this group
            route = CentroidRoute(
                name=group_name,
                centroid=np.array(centroid)
            )
            routes[group_name] = route
        
        logging.info(f"Set up {len(routes)} group routes")
        return routes
    
    def _setup_expert_routes(self) -> Dict[str, Dict[str, CentroidRoute]]:
        """
        Set up the expert-level routes based on centroids.
        
        Returns:
            Nested dictionary mapping group names to expert routes
        """
        routes = {}
        
        for group_name, experts in GROUP_TO_EXPERTS.items():
            routes[group_name] = {}
            
            for expert_name in experts:
                if expert_name in EXPERT_CENTROIDS:
                    # Create a CentroidRoute for this expert
                    route = CentroidRoute(
                        name=expert_name,
                        centroid=np.array(EXPERT_CENTROIDS[expert_name])
                    )
                    routes[group_name][expert_name] = route
        
        expert_count = sum(len(group_experts) for group_experts in routes.values())
        logging.info(f"Set up {expert_count} expert routes across {len(routes)} groups")
        return routes
    
    def register_expert_response(self, expert_name: str, response_func: Callable):
        """
        Register a response function for a specific expert.
        
        Args:
            expert_name: Name of the expert
            response_func: Function to handle queries for this expert
        """
        self.expert_responses[expert_name] = response_func
    
    async def _fallback_response(self, query: str):
        """
        Default fallback response when no matching expert is found.
        
        Args:
            query: The user query
            
        Returns:
            A fallback response
        """
        return {
            "answer": "I'm not sure which expert can best help with that question. "
                     "Could you please provide more specific details?"
        }
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """
        Get an embedding for the input text.
        
        Args:
            text: Input text to encode
            
        Returns:
            Embedding vector as numpy array
        """
        if self.use_openai:
            # Use OpenAI's encoder
            return self.encoder.encode(text)
        else:
            # Use SentenceTransformer
            return self.model.encode(text)
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors.
        
        Args:
            vec1: First vector
            vec2: Second vector
            
        Returns:
            Cosine similarity score
        """
        # Ensure both vectors have the same dtype (float32)
        vec1 = vec1.astype(np.float32)
        vec2 = vec2.astype(np.float32)
        return float(util.cos_sim(vec1.reshape(1, -1), vec2.reshape(1, -1))[0][0])
    
    async def route_query(self, query: str) -> Dict[str, Any]:
        """
        Route a query through the two-layer system:
        1. Find the best expert
        2. Retrieve relevant documents
        3. Generate response using LLM
        
        Args:
            query: The user query
            
        Returns:
            Response from the selected expert
        """
        # Get query embedding
        query_embedding = await self.get_embedding(query)
        
        # First-layer routing: find the best group
        best_group = None
        best_similarity = -1
        
        for group_name, route in self.group_routes.items():
            similarity = self.cosine_similarity(query_embedding, route.centroid)
            logging.info(f"Group '{group_name}' similarity: {similarity:.4f}")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_group = group_name
        
        if best_group is None:
            logging.warning("No matching group found")
            return await self.fallback_response(query)
        
        logging.info(f"Selected group: {best_group} (similarity: {best_similarity:.4f})")
        
        # Second-layer routing: find the best expert within the group
        best_expert = None
        best_similarity = -1
        
        start_time = time.time()
        for expert_name, route in self.expert_routes.get(best_group, {}).items():
            similarity = self.cosine_similarity(query_embedding, route.centroid)
            logging.info(f"Expert '{expert_name}' similarity: {similarity:.4f}")
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_expert = expert_name
        
        if best_expert is None:
            logging.warning(f"No matching expert found in group {best_group}")
            return await self.fallback_response(query)
        
        logging.info(f"Selected expert: {best_expert} (similarity: {best_similarity:.4f})")
        # log info in miliseconds
        logging.info(f"Time taken for routing: {(time.time() - start_time) * 1000:.2f} ms")
        
        # Check if custom response function is registered
        if best_expert in self.expert_responses:
            # Use custom response function
            return await self.expert_responses[best_expert](query)
        
        # Default RAG pipeline:
        # 1. Retrieve relevant documents from ChromaDB
        results = await self.db_handler.get_similar_documents(best_expert, query_embedding.tolist(), self.top_k)
        
        # 2. Extract documents
        documents = results.get("documents", [])
        
        if not documents:
            logging.warning(f"No documents found for expert {best_expert}")
            return await self.fallback_response(query)
        
        # 3. Generate response using LLM
        response = await generate_response(query, documents, best_expert)
        
        return response